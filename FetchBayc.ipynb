{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests\n",
        "from google.colab import drive, files\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "from __future__ import annotations\n",
        "import csv, datetime as dt, time, random\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import requests, sys\n",
        "from pprint import pprint\n",
        "\n",
        "ENDPOINT   = \"https://api.studio.thegraph.com/query/111374/bayc-mainnet/v0.0.1\"\n",
        "DRIVE_DIR  = Path(\"/content/drive/MyDrive/BAYC\")\n",
        "CSV_HEADERS = [\n",
        "    \"id\",\"blockNumber\",\"blockTimestamp\",\n",
        "    \"transactionHash\",\"tokenId\",\"from\",\"to\"\n",
        "]\n",
        "PAGE_SIZE = 1000\n",
        "REQUESTS_PER_SEC = 2\n",
        "MAX_ROWS = None\n",
        "\n",
        "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def gql(after_id: str|None) -> str:\n",
        "    cursor = f', where: {{ id_gt: \"{after_id}\" }}' if after_id else \"\"\n",
        "    return f\"\"\"\n",
        "{{\n",
        "  transfers(first: {PAGE_SIZE}{cursor},\n",
        "            orderBy: id, orderDirection: asc) {{\n",
        "    id\n",
        "    blockNumber\n",
        "    blockTimestamp\n",
        "    transactionHash\n",
        "    tokenId\n",
        "    from\n",
        "    to\n",
        "  }}\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "def fetch_batch(s: requests.Session, after_id: str|None) -> List[Dict]:\n",
        "    r = s.post(ENDPOINT, json={\"query\": gql(after_id)}, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    if \"errors\" in data:\n",
        "        pprint(data[\"errors\"]); raise RuntimeError(\"GraphQL error\")\n",
        "    return data[\"data\"][\"transfers\"]\n",
        "\n",
        "def append_csv(path: Path, rows: List[Dict], first_batch: bool):\n",
        "    mode = \"w\" if first_batch else \"a\"\n",
        "    with path.open(mode, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, CSV_HEADERS)\n",
        "        if first_batch: w.writeheader()\n",
        "        w.writerows(rows)\n",
        "\n",
        "def export(max_rows: int|None = None) -> Path:\n",
        "    ensure_dir(DRIVE_DIR)\n",
        "    ts = dt.datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "    out = DRIVE_DIR / f\"bayc_transfers_{ts}.csv\"\n",
        "\n",
        "    sess = requests.Session()\n",
        "    after, total, batch_no = None, 0, 0\n",
        "    while True:\n",
        "        if max_rows and total >= max_rows: break\n",
        "        rows = fetch_batch(sess, after)\n",
        "        if not rows:\n",
        "            print(f\"✅ DONE，TOTAL {total} ROWS\"); break\n",
        "        append_csv(out, rows, first_batch=(batch_no==0))\n",
        "        total += len(rows)\n",
        "        after = rows[-1][\"id\"]\n",
        "        print(f\"Batch {batch_no:>4} ✔ {len(rows):>4} rows → 总计 {total}\")\n",
        "        batch_no += 1\n",
        "        time.sleep(random.uniform(1/REQUESTS_PER_SEC, 1.5/REQUESTS_PER_SEC))\n",
        "    return out\n",
        "\n",
        "csv_path = export(MAX_ROWS)\n",
        "print(f\"\\nCSV Has been saved to：{csv_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SlR2YE5EhJuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SRC = Path(\"/content/drive/MyDrive/BAYC/bayc_transfers_20250512_2125.csv\")\n",
        "DST = SRC.with_name(SRC.stem + \"_clean.csv\")\n",
        "\n",
        "df = pd.read_csv(SRC)\n",
        "\n",
        "ts_col = \"blockTimestamp\" if \"blockTimestamp\" in df.columns else \"timestamp\"\n",
        "df[ts_col] = pd.to_datetime(df[ts_col], unit=\"s\", utc=True)\n",
        "\n",
        "if \"value\" in df.columns:\n",
        "    df[\"valueEth\"] = df[\"value\"] / 1e18\n",
        "\n",
        "dedup_cols = [c for c in (\"transactionHash\", \"txHash\", \"id\") if c in df.columns]\n",
        "if \"logIndex\" in df.columns:\n",
        "    dedup_cols.append(\"logIndex\")\n",
        "\n",
        "if dedup_cols:\n",
        "    df = df.drop_duplicates(subset=dedup_cols, keep=\"first\")\n",
        "else:\n",
        "    print(\"⚠️ Didn't find txHash / transactionHash / id ...rows，Skip\")\n",
        "\n",
        "df = df.sort_values(ts_col).reset_index(drop=True)\n",
        "df.to_csv(DST, index=False)\n",
        "\n",
        "print(f\"✅ Done：{len(df):,} Rows → {DST}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh3_4sI0mjok",
        "outputId": "6e6e1e04-a0dc-4f9f-e6d1-05a85b55021d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done：306,306 Rows → /content/drive/MyDrive/BAYC/bayc_transfers_20250512_2125_clean.csv\n"
          ]
        }
      ]
    }
  ]
}